# Phase Final Implementation Guide — Step‑by‑Step Plan

This guide turns the requirements and solution design into a staged implementation plan, optimized for small, debuggable increments.

## 1. Milestones Overview

1. **M1 — Preprocessing & Feature Extraction (Local, No Containers)**
   - Implement a high‑throughput preprocessing pipeline that:
     - Normalizes originals and generates thumbnails / web‑friendly versions.
     - Extracts metadata (time, file stats, EXIF, GPS).
     - Computes fingerprints (hashes, perceptual hashes) and near‑duplicate groups.
     - Runs the initial perception models (SigLIP/BLIP and, where feasible, detection).
   - Store all results in caches and a SQLite database for reuse.
2. **M2 — Search & Tools (SQLite + Local Services)**
   - Build CLI and minimal UI on top of the SQLite database from M1.
   - Provide search, filtering, and basic annotation operations without containers.
3. **M3 — Database & Deployment Upgrade (PostgreSQL + pgvector + docker-compose)**
   - Move from SQLite to PostgreSQL/pgvector.
   - Containerize the stack with `docker-compose`.
   - Introduce Redis and background worker stack.
4. **M4 — Learning & Personalization**
   - Implement few‑shot learning, corrections loop, and batch annotation workflows.
   - Add basic monitoring and operational tooling.

Each milestone should be shippable and testable in isolation, and early milestones should run directly on the host via `uv` rather than containers.

## 2. Repository Layout

Recommended layout under `src/`:

```text
src/
├── api/               # FastAPI routers, schemas
├── background/        # Celery tasks, scheduling
├── core/              # Core domain/model abstractions
├── data/              # Repositories, migrations, DB access
├── ml/                # Model wrappers, detection & embedding pipelines
├── ui/                # Streamlit frontend
└── utils/             # Logging, configuration, helpers
```

Supporting directories (see `DIRECTORY_STRUCTURE.md`):

- `data/` — Runtime DB files for SQLite (M1/M2) and optional exports.
- `cache/` — Normalized images, thumbnails, detections, captions, embeddings.
- `models/` — Downloaded model weights and tokenizer/processor assets.
- `log/` — Structured application logs.
- `tmp/` — Temporary files.

## 3. Milestone Details

### 3.1 M1 — Preprocessing & Feature Extraction (Local, No Containers)

Goal: Build a robust, efficient pipeline that can process tens of thousands of photos and cache all reusable results.

- Tooling:
  - Use Python 3.12 with `uv` for environment management.
  - Configure `black` + `ruff` as per `AI_CODING_STANDARDS.md`.
  - Implement a shared logging module (structured logs, no `print` in production code).
- Configuration:
  - Parse `config/settings.yaml` (generated by `./init_project.sh`).
  - Support configuration of:
    - Input photo roots (local folders, NAS mounts).
    - Output directories for caches and thumbnails.
    - Model choices and batch sizes.
- Database (SQLite):
  - Design a minimal schema in a local SQLite DB under `data/`.
  - Tables for:
    - Photos (paths, file hashes, perceptual hashes, timestamps, EXIF).
    - Basic labels/captions/embeddings from models:
      - Store SigLIP image embeddings and coarse category outputs (`primary_category`, coarse category scores) in the DB.
      - Do **not** cache classification results separately under `cache/`; treat them as derived projections over cached embeddings/detections.
    - Optional detection outputs if models are enabled.
- Image preprocessing:
  - Normalize images (orientation, color profile) and generate:
    - Web‑friendly thumbnails (e.g. 512×512).
    - Lower‑quality versions for fast display.
- Metadata extraction:
  - Extract:
    - Capture time (EXIF if available; otherwise fallback to file creation/modification times).
    - GPS/geolocation data when present.
    - File size, dimensions, and format.
- Fingerprints & deduplication:
  - Compute:
    - File hash (e.g. SHA‑256).
    - Perceptual hash for near‑duplicate detection.
  - Identify near‑duplicate groups:
    - Record relationships in the DB.
    - Optionally skip heavy model inference for images deemed “near identical” while linking them to a canonical representative.
- Model inference:
  - Load models once per process and reuse them across images to avoid reload overhead.
  - Start with:
    - SigLIP embeddings (full image), including coarse category classification against a small, fixed set (food, electronics, document, screenshot, people, landscape, other, or similar).
    - BLIP captions.
  - Where feasible (depending on hardware and priorities), add:
    - Grounding DINO / OWL‑ViT detection and SigLIP re‑ranking.
  - Store all outputs in the SQLite DB and JSON caches under `cache/`.
- Concurrency & robustness:
  - Use a simple queue and worker model (e.g. multiprocessing / threading) appropriate for local runs.
  - Support resumable processing:
    - Skip images that already have valid cached results.
    - Record progress and failures for later inspection.
  - Keep preprocessing caches (JSON/NPY under `cache/`) as the primary durable artifact so that future milestones can rebuild or change database schemas quickly using these cached results.
- Debugging UI:
  - Implement a simple Flask‑based web UI (for M1 only) to:
    - List all photos that were fully processed, excluding those skipped as near‑duplicates.
    - Show a detail page per photo with all extracted preprocessing information (metadata, hashes, EXIF/GPS, model outputs).
    - List all similar/near‑duplicate photos linked to that photo.

#### 3.1.1 M1 Preparation Status (Current)

Several pieces of the M1 stack are already in place and can be reused directly:

- Configuration and documentation:
  - `config/settings.example.yaml` defines model configuration (embedding, caption, detection, OCR) and core pipeline flags (`run_detection`, `skip_duplicates_for_heavy_models`).
  - `docs/MODELS.md` specifies concrete model choices for M1 (SigLIP2 + BLIP, optional OWL‑ViT) and how they are used.
  - `docs/AI_CODING_NOTES.md` refines `AI_CODING_STANDARDS.md` with model integration guidelines (Transformers APIs only, singletons, batching, CPU‑first device selection).
- Model helpers:
  - `src/vibe_photos/ml/coarse_categories.py` implements a model‑agnostic coarse category classifier and a SigLIP‑specific wiring helper.
  - `src/vibe_photos/ml/model_presets.py` centralizes canonical Hugging Face model IDs and exposes named presets for SigLIP2 and BLIP.
  - `src/vibe_photos/config.py` provides typed configuration (`Settings`) and a defensive loader for `config/settings.yaml`, including model and pipeline sections.
  - `src/vibe_photos/ml/models.py` exposes singleton loaders for SigLIP embeddings and BLIP captioning, including automatic device selection (`auto` → CUDA/MPS/CPU).
- Developer tools:
  - `tools/test_coarse_categories.py` smoke‑tests SigLIP‑based coarse categories using the global configuration by default, with a `--model-name` override for ad‑hoc experiments.

Together, these components mean that M1 implementation can focus on the preprocessing and storage pipeline, rather than low‑level model/wiring concerns.

#### 3.1.2 Recommended Next Steps for M1 Implementation

To start M1 development, build on the existing groundwork in the following order:

1. **Finalize configuration and presets**
   - Copy `config/settings.example.yaml` to `config/settings.yaml` and tune:
     - Photo roots and cache/output paths.
     - Embedding and caption model presets (`m1_default`, `caption_base`) and batch sizes.
     - Initial detection/OCR flags (`enabled`, `run_detection`).
   - Confirm that `load_settings()` and `get_*_model_name()` in `src/vibe_photos/config.py` behave as expected on the target development machine.
2. **Implement the core preprocessing pipeline**
   - Create a dedicated preprocessing module under `src/vibe_photos/ml/` or `src/vibe_photos/core/` that:
     - Walks configured photo roots.
     - Normalizes images and writes thumbnails into `cache/`.
     - Computes hashes/perceptual hashes and stores them in SQLite (`data/`).
   - Integrate SigLIP and BLIP via `get_siglip_embedding_model()` and `get_blip_caption_model()` (`src/vibe_photos/ml/models.py`):
     - Batch images using the configured batch sizes.
     - Persist embeddings and captions both to caches (`cache/`) and to the SQLite schema.
3. **Wire coarse categories into the pipeline**
   - Reuse `build_siglip_coarse_classifier()` (`src/vibe_photos/ml/coarse_categories.py`) to compute:
     - `primary_category` and coarse category scores as DB fields.
     - Optional derived fields to gate detection/OCR work (for example, only run detection for electronics/food/document/screenshots).
4. **Design and implement the initial SQLite schema**
   - Start from the M1 subset of the Phase Final database design:
     - Photos, embeddings, captions, coarse categories, near‑duplicate relations.
   - Keep schema changes localized and treat caches under `cache/` as the durable source of truth for recomputing databases.
5. **Add a minimal CLI for running M1**
   - Implement a Typer‑based CLI under `src/` to:
     - Trigger preprocessing on configured roots.
     - Inspect basic statistics (number of processed photos, missing metadata, error counts).
   - Keep the Flask debug UI (described above) as a secondary, nice‑to‑have after the core CLI path is working.

Once these steps are complete and validated on a real subset of photos, M1 can be considered ready for broader experimentation and evaluation, and the focus can shift to M2 search and tools.

#### 3.1.3 System Prompt Template for M1 Coding AIs

When starting a new session with a coding AI (e.g. Codex, GPT‑4 class models) to work on M1, use the following prompt as a **system‑level template**. It encodes the Phase Final goals, model choices, existing scaffolding, and coding constraints described above.

```text
You are an AI coding assistant working on the repository `jasl/vibe_photos_v4`.

## Goal

Implement Phase Final — M1 (Preprocessing & Feature Extraction) for a local‑first photo intelligence system.

M1 must:

1. Scan local photo folders and register photos (path, hashes, timestamps, EXIF, etc.).
2. Generate normalized images and thumbnails.
3. Compute:
   - SigLIP image embeddings (for vector search and coarse categories).
   - BLIP captions (one short caption per image).
4. Optionally run open‑vocabulary detection (OWL‑ViT) if enabled in config.
5. Store all results in:
   - Versioned caches under `cache/`.
   - A SQLite database for search and inspection.

The system MUST run fully on a local PC/Mac/NAS (CPU‑only is allowed; GPU is a bonus).

## Project Documents (source of truth)

You MUST follow these documents and keep your code consistent with them:

- `blueprints/phase_final/docs/01_requirements.md` — product requirements and quality targets.
- `blueprints/phase_final/docs/02_solution_design.md` — architecture and workflows.
- `blueprints/phase_final/docs/03_technical_choices.md` — runtime stack and model strategy.
- `blueprints/phase_final/docs/04_implementation_guide.md` — milestones, step‑by‑step plan, and M1 status.
- `blueprints/phase_final/docs/05_siglip_blip_integration.md` — detection + SigLIP + BLIP data flow.
- `docs/MODELS.md` — concrete model IDs and integration notes.
- `config/settings.example.yaml` — example configuration for models and the pipeline.
- `AI_CODING_STANDARDS.md` — global coding standards.
- `docs/AI_CODING_NOTES.md` — model integration guidelines (Transformers usage, singletons, batching, caching).

Before writing code for a module, briefly restate which parts of these documents you are following.

## Existing Building Blocks (reuse them)

Do NOT re‑invent model helpers or settings; build on the existing modules:

- `src/vibe_photos/ml/coarse_categories.py`
  - `CoarseCategory`, `CoarseCategoryClassifier`
  - `DEFAULT_COARSE_CATEGORIES`
  - `build_siglip_coarse_classifier(...)`
- `src/vibe_photos/ml/model_presets.py`
  - Canonical model IDs:
    - SigLIP2 base: `SIGLIP2_BASE_PATCH16_224 = "google/siglip2-base-patch16-224"`
    - SigLIP2 large: `SIGLIP2_LARGE_PATCH16_384`
    - BLIP base: `BLIP_IMAGE_CAPTIONING_BASE = "Salesforce/blip-image-captioning-base"`
    - BLIP large: `BLIP_IMAGE_CAPTIONING_LARGE`
  - Preset maps: `SIGLIP_PRESETS`, `BLIP_PRESETS`
- `src/vibe_photos/config.py`
  - Dataclasses: `EmbeddingModelConfig`, `CaptionModelConfig`, `DetectionModelConfig`, `OcrConfig`, `ModelsConfig`, `PipelineConfig`, `Settings`.
  - Loader: `load_settings()` and helpers such as `get_embedding_model_name()`, `get_caption_model_name()`.
- `src/vibe_photos/ml/models.py`
  - Device selection with `_select_device("auto")` (CUDA → MPS → CPU).
  - Singletons:
    - `get_siglip_embedding_model(settings)`
    - `get_blip_caption_model(settings)`
- `tools/test_coarse_categories.py`
  - CLI tool to smoke‑test SigLIP coarse categories using global config by default, with a `--model-name` override.

Whenever you need SigLIP or BLIP, prefer `load_settings()` + `get_siglip_embedding_model()` / `get_blip_caption_model()` instead of creating new loaders.

## Model Choices (M1 focus)

Follow `docs/MODELS.md` and `src/vibe_photos/ml/model_presets.py`. Do NOT change model IDs or presets unless explicitly asked.

- Embeddings and zero‑shot classification (SigLIP2):
  - Default (M1): `google/siglip2-base-patch16-224` (preset `"m1_default"`).
  - High‑quality (later): `google/siglip2-large-patch16-384` (preset `"hq_384"`).
- Captioning (BLIP):
  - Default (M1): `Salesforce/blip-image-captioning-base` (preset `"caption_base"`).
  - Optional high‑quality: `Salesforce/blip-image-captioning-large` (preset `"caption_large"`).
- Detection (optional in M1):
  - Backend: `owlvit`.
  - Model: `google/owlvit-base-patch32`.
  - M1 can ship with detection disabled (`models.detection.enabled=false`, `pipeline.run_detection=false`).

OCR and few‑shot learning are out of scope for M1 (interfaces only, no concrete OCR implementation).

## Critical Coding Constraints

You MUST respect these constraints (see `AI_CODING_STANDARDS.md` and `docs/AI_CODING_NOTES.md`):

1. Transformers API only
   - Use `AutoProcessor`, `AutoModel`, `AutoModelForSeq2SeqLM`.
   - Do NOT use legacy pipelines such as `pipeline("image-classification", model=...)` or old CLIP APIs for SigLIP.
2. Single model instance per process
   - Use the singleton loaders in `src/vibe_photos/ml/models.py`.
   - Never load models inside tight loops or per image.
3. CPU‑first, GPU‑enhanced
   - Code MUST run correctly on CPU‑only machines.
   - If CUDA/MPS is available, use it via `_select_device("auto")`, but never require it.
4. Config‑driven
   - All model names, presets, batch sizes, and feature flags come from `config/settings.yaml`.
   - Use `Settings` from `src/vibe_photos/config.py` instead of ad‑hoc config parsing.
5. Batching
   - Implement batching for SigLIP and BLIP using `models.embedding.batch_size` and `models.caption.batch_size`.
   - Detection batching is optional in M1.
6. Caching and DB separation
   - Treat caches under `cache/` (JSON/NPY, with model and pipeline version info) as the durable source of truth.
   - SQLite (under `data/`) is a projection over caches and may be rebuilt.
7. Modern Python style
   - Python 3.12, type hints, `dataclasses`, `pathlib.Path`.
   - Follow import order and logging rules from `AI_CODING_STANDARDS.md`.

## High‑Level Tasks for M1

Work in small, testable steps. For each step:

1. Explain which documents/sections you are following.
2. Note which existing modules you are extending.
3. Propose module/function names.
4. Write the code.
5. Add minimal tests or usage examples where feasible.

Priority steps:

1. Config and paths
   - Ensure `Settings` covers input roots, cache directory, DB path, model settings, and pipeline flags.
   - Add a CLI or script to print the effective config.
2. Model loading
   - Reuse `get_siglip_embedding_model()` and `get_blip_caption_model()`.
   - Optionally add `get_detector()` based on `models.detection` config.
3. Preprocessing and hashing
   - Implement a preprocessing module that:
     - Walks configured photo roots.
     - Normalizes images and writes thumbnails under `cache/`.
     - Computes hashes/perceptual hashes and writes them to caches and SQLite.
4. Embeddings and coarse categories
   - Use SigLIP to compute embeddings in batches and classify coarse categories via `build_siglip_coarse_classifier()`.
5. Captions
   - Use BLIP to generate one short caption per image and store it in caches and SQLite.
6. (Optional) Detection
   - Add a pluggable OWL‑ViT backend controlled by config flags.
7. CLI and status
   - Provide commands to run preprocessing and inspect status for M1.

## Output style

- Prefer clear, well‑structured modules over huge scripts.
- Use docstrings and type hints.
- When adding a new file, always show its relative path.
- When unsure about details not covered by docs, make a reasonable assumption and document it.
```

### 3.2 M2 — Search & Tools (SQLite + Local Services)

Goal: Expose the preprocessed data through search APIs and simple tools, still without containers.

- API and CLI:
  - Implement FastAPI endpoints backed by SQLite for:
    - Searching by text and filters.
    - Querying photo details and metadata.
  - Provide CLI commands for:
    - Running preprocessing on demand.
    - Executing searches and exporting results.
- Minimal UI:
  - Build a small Streamlit UI:
    - Search bar, filters (time, category, collections).
    - Grid of thumbnails with captions and key labels.
    - Detail view showing extracted metadata.
- Evaluation:
  - Run on real subsets of user photos to validate:
    - Preprocessing correctness (timestamps, GPS, deduplication).
    - Search usefulness for electronics, food, documents, and screenshots.
  - For M1 exit, run a small manual evaluation on ~100–200 diverse photos (electronics, food, documents, screenshots, people, landscapes):
    - Hand‑label expected coarse categories and 1–3 key objects per photo.
    - Verify that:
      - Coarse category accuracy reaches at least ~85% on this sample.
      - Key objects are discoverable via search (either via labels or captions).

### 3.3 M3 — Database & Deployment Upgrade (PostgreSQL + pgvector + docker-compose)

Goal: Move to the final database and deployment architecture.

- Database:
  - Define PostgreSQL + pgvector schema based on `../specs/database_schema.sql` (adapted to PostgreSQL syntax).
  - Implement migrations via Alembic or a similar tool.
  - Implement a data migration path from SQLite (where feasible).
- Vector search:
  - Create pgvector columns for embeddings and region embeddings.
  - Build indexes and tune parameters for 30k–100k photos.
- `docker-compose` stack:
  - Define services for:
    - PostgreSQL + pgvector.
    - Redis.
    - API.
    - Workers.
    - UI.
  - Provide environment templates (`.env.example`) and simple setup scripts.
- Background processing:
  - Promote ingestion tasks into a proper queue (Celery + Redis).
  - Add retry logic, simple metrics, and error reporting.

### 3.4 M4 — Learning & Personalization

Goal: Turn corrections and examples into a self‑improving system.

- Few‑shot learning:
  - Implement a prototype manager (similar to `few_shot_learner.py` PoC) using real embeddings.
  - UI flow:
    - “Teach a new product” wizard.
    - Show estimated accuracy and thresholds.
- Corrections loop:
  - Log all human corrections and accepted suggestions.
  - Surface frequently overridden labels and patterns.
  - Feed these signals into ranking and few‑shot prototypes.
- Batch operations:
  - Enable “apply label to similar photos” operations.
  - Provide safety checks and undo options where possible.
- Monitoring & Maintenance:
  - Expose basic Prometheus metrics for ingestion throughput and failures.
  - Add backup/restore scripts for PostgreSQL.

## 4. Deployment Notes

- For development:
  - Use `uv` to manage the Python environment.
  - Run preprocessing, search, and evaluation scripts via `uv run` outside Docker in M1–M2.
- For user deployment:
  - Prefer `docker-compose` as the single entry point (M3+).
  - Ensure clear documentation:
    - How to mount photo directories.
    - How to mount `models/`, `cache/`, and `data/`.
    - How to update the stack safely.

Track progress in `AI_TASK_TRACKER.md` and record design decisions and deviations in `decisions/AI_DECISION_RECORD.md`.
