# Phase Final Implementation Guide — Step‑by‑Step Plan

This guide turns the requirements and solution design into a staged implementation plan, optimized for small, debuggable increments.

## 1. Milestones Overview

1. **M1 — Preprocessing & Feature Extraction (Local, No Containers)**
   - Implement a high‑throughput preprocessing pipeline that:
     - Normalizes originals and generates thumbnails / web‑friendly versions.
     - Extracts metadata (time, file stats, EXIF, GPS).
     - Computes fingerprints (hashes, perceptual hashes) and near‑duplicate groups.
     - Runs the initial perception models (SigLIP/BLIP and, where feasible, detection).
   - Store all results in caches and a SQLite database for reuse.
2. **M2 — Perception Quality & Labeling**
   - Improve recognition quality and label hygiene on top of M1:
     - Iterate on SigLIP label sets and grouping to reduce manual maintenance and improve coverage.
     - Refine detection + SigLIP/BLIP thresholds, blacklists, and remapping to reduce noisy object/product labels.
     - Establish small but realistic evaluation sets and metrics for coarse categories and object‑level recognition.
3. **M3 — Search & Tools (PostgreSQL + pgvector + docker-compose)**
   - Move from SQLite to PostgreSQL/pgvector and build search/tools on top of the improved perception stack.
   - Containerize the stack with `docker-compose` (API, workers, DB, Redis, UI).
   - Introduce Redis and background worker stack.
4. **M4 — Learning & Personalization**
   - Implement few‑shot learning, corrections loop, and batch annotation workflows.
   - Add basic monitoring and operational tooling.

Each milestone should be shippable and testable in isolation, and early milestones should run directly on the host via `uv` rather than containers.

## 2. Repository Layout

Recommended layout under `src/`:

```text
src/
├── api/               # FastAPI routers, schemas
├── background/        # Celery tasks, scheduling
├── core/              # Core domain/model abstractions
├── data/              # Repositories, migrations, DB access
├── ml/                # Model wrappers, detection & embedding pipelines
├── ui/                # Streamlit frontend
└── utils/             # Logging, configuration, helpers
```

Supporting directories (see `DIRECTORY_STRUCTURE.md`):

- `data/` — Runtime DB files for SQLite (M1/M2) and optional exports.
- `cache/` — Normalized images, thumbnails, detections, captions, embeddings.
- `models/` — Downloaded model weights and tokenizer/processor assets.
- `log/` — Structured application logs.
- `tmp/` — Temporary files.

## 3. Milestone Details

### 3.1 M1 — Preprocessing & Feature Extraction (Local, No Containers)

Goal: Build a robust, efficient pipeline that can process tens of thousands of photos and cache all reusable results. This repository already implements that pipeline; new work should treat it as the baseline for later milestones.

- Tooling:
  - Use Python 3.12 with `uv` for environment management.
  - Configure `black` + `ruff` as per `AI_CODING_STANDARDS.md`.
  - Implement a shared logging module (structured logs, no `print` in production code).
- Configuration:
  - Parse `config/settings.yaml` (generated by `./init_project.sh`).
  - Support configuration of:
    - Input photo roots (local folders, NAS mounts).
    - Output directories for caches and thumbnails.
    - Model choices and batch sizes.
- Database (SQLite):
  - Follow the canonical M1 schema defined in `blueprints/m1/m1_development_plan.md`:
    - A primary operational database under `data/index.db` for canonical image metadata (`images` table with content hash and perceptual hash fields).
    - A projection database under `cache/index.db` for:
      - Lightweight scene classification outputs (`image_scene`).
      - Embedding and caption projections (`image_embedding`, `image_caption`).
      - Near-duplicate relationships (`image_near_duplicate`) derived from `images.phash`.
  - All projection tables are rebuildable from caches under `cache/` and the primary database; treat SQLite as a projection layer over durable caches.
- Image preprocessing:
  - Normalize images (orientation, color profile) and generate:
    - Web‑friendly thumbnails (e.g. 512×512).
    - Lower‑quality versions for fast display.
- Metadata extraction:
  - Extract:
    - Capture time (EXIF if available; otherwise fallback to file creation/modification times).
    - GPS/geolocation data when present.
    - File size, dimensions, and format.
- Fingerprints & deduplication:
  - Compute:
    - Content hash using a 64-bit xxHash (`xxhash64-v1`) over the raw file bytes, stored as a 16-character hexadecimal `image_id`.
    - Perceptual hash (`phash64-v2`) using a fixed 64-bit DCT-based pHash (32×32 grayscale → 8×8 low-frequency block → median threshold) for near-duplicate detection, following `blueprints/m1/m1_development_plan.md`.
  - Identify near-duplicate groups:
    - Treat two active images as near-duplicates when the Hamming distance between their 64-bit `phash` values is less than or equal to a configurable threshold (see `Settings.pipeline.phash_hamming_threshold`, default 12 in this codebase).
    - Record near-duplicate relationships in `image_near_duplicate` (for SQLite) or the equivalent table in PostgreSQL/pgvector, designating an anchor/canonical image and linking duplicates to it.
    - Optionally use high-order `phash` bits (for example, the top 16 bits) as buckets and only compute full Hamming distances within buckets to keep grouping efficient at 30k+ photos; experiments in M1 showed that a simple all-pairs scan is acceptable at current library sizes but bucketing remains a future optimization.
    - Optionally skip heavy model inference for images deemed “near identical” while linking them to a canonical representative; in M1 this behavior is controlled by the `pipeline.skip_duplicates_for_heavy_models` flag in `config/settings.yaml`.
- Model inference:
  - Load models once per process and reuse them across images to avoid reload overhead.
  - Start with:
    - SigLIP embeddings (full image), including coarse category classification against a small, fixed set (food, electronics, document, screenshot, people, landscape, other, or similar).
    - BLIP captions.
  - Where feasible (depending on hardware and priorities), add:
    - Grounding DINO / OWL‑ViT detection and SigLIP re‑ranking.
  - Store all outputs in the SQLite DB and JSON caches under `cache/`.
- Concurrency & robustness:
  - Provide both a single-process CLI (`vibe_photos.dev.process`) and an optional Celery-based worker model (`vibe_photos.task_queue` + `vibe_photos.dev.enqueue_celery`) appropriate for local runs.
  - Support resumable processing:
    - Skip images that already have valid cached results.
    - Record progress and failures for later inspection.
  - Keep preprocessing caches (JSON/NPY under `cache/`) as the primary durable artifact so that future milestones can rebuild or change database schemas quickly using these cached results.
  - Persist a lightweight run journal under `cache/run_journal.json` (stage name, last processed cursor, checkpoint time) so long runs can resume after interruption, following the M1 blueprint.
- Debugging UI:
  - Implement a simple Flask‑based web UI (for M1 only) to:
    - List all photos that were fully processed, excluding those skipped as near‑duplicates.
    - Show a detail page per photo with all extracted preprocessing information (metadata, hashes, EXIF/GPS, model outputs) without exposing absolute filesystem paths or `file://` URIs.
    - List all similar/near‑duplicate photos linked to that photo.

#### 3.1.1 M1 Implementation Status (Current)

Several pieces of the M1 stack are already in place and can be reused directly:

- Configuration and documentation:
  - `config/settings.example.yaml` defines model configuration (embedding, caption, detection, OCR) and core pipeline flags (`run_detection`, `skip_duplicates_for_heavy_models`).
  - `docs/MODELS.md` specifies concrete model choices for M1 (SigLIP2 + BLIP, optional OWL‑ViT) and how they are used.
  - `docs/AI_CODING_NOTES.md` refines `AI_CODING_STANDARDS.md` with model integration guidelines (Transformers APIs only, singletons, batching, CPU‑first device selection).
- Model helpers:
  - `src/vibe_photos/ml/coarse_categories.py` implements a model‑agnostic coarse category classifier and a SigLIP‑specific wiring helper.
  - `src/vibe_photos/ml/model_presets.py` centralizes canonical Hugging Face model IDs and exposes named presets for SigLIP2 and BLIP.
  - `src/vibe_photos/config.py` provides typed configuration (`Settings`) and a defensive loader for `config/settings.yaml`, including model and pipeline sections.
  - `src/vibe_photos/ml/models.py` exposes singleton loaders for SigLIP embeddings and BLIP captioning, including automatic device selection (`auto` → CUDA/MPS/CPU).
- Developer tools:
  - `tools/test_coarse_categories.py` smoke‑tests SigLIP‑based coarse categories using the global configuration by default, with a `--model-name` override for ad‑hoc experiments.
- Core implementation:
  - `src/vibe_photos/hasher.py`, `src/vibe_photos/scanner.py`, and `src/vibe_photos/pipeline.py` implement content/perceptual hashing, filesystem scanning, and the high-level `PreprocessingPipeline` orchestration (including cache manifest and run journal).
  - `src/vibe_photos/preprocessing.py` and `src/vibe_photos/artifact_store.py` define shared preprocessing steps and an artifact manager used by both Celery workers and the single-image helper.
  - `src/vibe_photos/dev/process.py` exposes the recommended single-process CLI for running the full M1 pipeline (`uv run python -m vibe_photos.dev.process ...`).
  - `src/vibe_photos/task_queue.py` and `src/vibe_photos/dev/enqueue_celery.py` wire Celery + Redis queues for preprocessing, main-stage, and enhancement tasks on top of the same preprocessing primitives.
  - `src/vibe_photos/webui/` implements the Flask-based debug UI used to inspect canonical photos, near-duplicate groups, and preprocessing outputs.

Together, these components mean that new work can focus on search, APIs, and deployment rather than re-implementing the M1 preprocessing and storage pipeline.

#### 3.1.2 Recommended Next Steps for M1 Implementation

To start M1 development, build on the existing groundwork in the following order:

1. **Finalize configuration and presets**
   - Copy `config/settings.example.yaml` to `config/settings.yaml` and tune:
     - Photo roots and cache/output paths.
     - Embedding and caption model presets (`m1_default`, `caption_base`) and batch sizes.
     - Initial detection/OCR flags (`enabled`, `run_detection`).
   - Confirm that `load_settings()` and `get_*_model_name()` in `src/vibe_photos/config.py` behave as expected on the target development machine.
2. **Implement the core preprocessing pipeline**
   - Create a dedicated preprocessing module under `src/vibe_photos/ml/` or `src/vibe_photos/core/` that:
     - Walks configured photo roots.
     - Normalizes images and writes thumbnails into `cache/`.
     - Computes content hashes and perceptual hashes following the M1 blueprint (`xxhash64-v1` over file bytes for `image_id`, `phash64-v2` for perceptual hash) and stores them in the `images` table in the primary SQLite database (`data/index.db`).
   - Integrate SigLIP and BLIP via `get_siglip_embedding_model()` and `get_blip_caption_model()` (`src/vibe_photos/ml/models.py`):
     - Batch images using the configured batch sizes.
     - Persist embeddings and captions both to caches (`cache/`) and to the SQLite schema.
3. **Wire coarse categories into the pipeline**
   - Reuse `build_siglip_coarse_classifier()` (`src/vibe_photos/ml/coarse_categories.py`) to compute:
     - `primary_category` and coarse category scores as DB fields.
     - Optional derived fields to gate detection/OCR work (for example, only run detection for electronics/food/document/screenshots).
4. **Design and implement the initial SQLite schema**
   - Start from the canonical M1 schema in `blueprints/m1/m1_development_plan.md`:
     - `images` in `data/index.db` (paths, `image_id` content hash, `phash`/`phash_algo`/`phash_updated_at`, timestamps, EXIF).
     - `image_scene`, `image_embedding`, `image_caption`, and `image_near_duplicate` in `cache/index.db` as projections over caches and `images`.
   - Keep schema changes localized and treat caches under `cache/` as the durable source of truth for recomputing databases.
5. **Add a minimal CLI for running M1**
   - Implement a Typer‑based CLI under `src/` to:
     - Trigger preprocessing on configured roots.
     - Inspect basic statistics (number of processed photos, missing metadata, error counts).
   - Keep the Flask debug UI (described above) as a secondary, nice‑to‑have after the core CLI path is working.

Once these steps are complete and validated on a real subset of photos, M1 can be considered ready for broader experimentation and evaluation, and the focus can shift to M2 search and tools.

#### 3.1.3 System Prompt Template for M1 Coding AIs

When starting a new session with a coding AI (e.g. Codex, GPT‑4 class models) to work on M1, use the following prompt as a **system‑level template**. It encodes the Phase Final goals, model choices, existing scaffolding, and coding constraints described above.

```text
You are an AI coding assistant working on the repository `jasl/vibe_photos_v4`.

## Goal

Implement Phase Final — M1 (Preprocessing & Feature Extraction) for a local‑first photo intelligence system.

M1 must:

1. Scan local photo folders and register photos (path, content hash, perceptual hash, timestamps, EXIF, etc.) following the M1 blueprint (`xxhash64-v1` for `image_id`, `phash64-v2` for `phash`).
2. Generate normalized images and thumbnails.
3. Compute:
   - SigLIP image embeddings (for vector search and coarse categories).
   - BLIP captions (one short caption per image).
4. Optionally run open‑vocabulary detection (OWL‑ViT) if enabled in config.
5. Store all results in:
   - Versioned caches under `cache/`.
   - A SQLite database for search and inspection.

The system MUST run fully on a local PC/Mac/NAS (CPU‑only is allowed; GPU is a bonus).

## Project Documents (source of truth)

You MUST follow these documents and keep your code consistent with them:

- `blueprints/phase_final/docs/01_requirements.md` — product requirements and quality targets.
- `blueprints/phase_final/docs/02_solution_design.md` — architecture and workflows.
- `blueprints/phase_final/docs/03_technical_choices.md` — runtime stack and model strategy.
- `blueprints/phase_final/docs/04_implementation_guide.md` — milestones, step‑by‑step plan, and M1 status.
- `blueprints/phase_final/docs/05_siglip_blip_integration.md` — detection + SigLIP + BLIP data flow.
- `docs/MODELS.md` — concrete model IDs and integration notes.
- `config/settings.example.yaml` — example configuration for models and the pipeline.
- `AI_CODING_STANDARDS.md` — global coding standards.
- `docs/AI_CODING_NOTES.md` — model integration guidelines (Transformers usage, singletons, batching, caching).

Before writing code for a module, briefly restate which parts of these documents you are following.

## Existing Building Blocks (reuse them)

Do NOT re‑invent model helpers or settings; build on the existing modules:

- `src/vibe_photos/ml/coarse_categories.py`
  - `CoarseCategory`, `CoarseCategoryClassifier`
  - `DEFAULT_COARSE_CATEGORIES`
  - `build_siglip_coarse_classifier(...)`
- `src/vibe_photos/ml/model_presets.py`
  - Canonical model IDs:
    - SigLIP2 base: `SIGLIP2_BASE_PATCH16_224 = "google/siglip2-base-patch16-224"`
    - SigLIP2 large: `SIGLIP2_LARGE_PATCH16_384`
    - BLIP base: `BLIP_IMAGE_CAPTIONING_BASE = "Salesforce/blip-image-captioning-base"`
    - BLIP large: `BLIP_IMAGE_CAPTIONING_LARGE`
  - Preset maps: `SIGLIP_PRESETS`, `BLIP_PRESETS`
- `src/vibe_photos/config.py`
  - Dataclasses: `EmbeddingModelConfig`, `CaptionModelConfig`, `DetectionModelConfig`, `OcrConfig`, `ModelsConfig`, `PipelineConfig`, `Settings`.
  - Loader: `load_settings()` and helpers such as `get_embedding_model_name()`, `get_caption_model_name()`.
- `src/vibe_photos/ml/models.py`
  - Device selection with `_select_device("auto")` (CUDA → MPS → CPU).
  - Singletons:
    - `get_siglip_embedding_model(settings)`
    - `get_blip_caption_model(settings)`
- `tools/test_coarse_categories.py`
  - CLI tool to smoke‑test SigLIP coarse categories using global config by default, with a `--model-name` override.

Whenever you need SigLIP or BLIP, prefer `load_settings()` + `get_siglip_embedding_model()` / `get_blip_caption_model()` instead of creating new loaders.

## Model Choices (M1 focus)

Follow `docs/MODELS.md` and `src/vibe_photos/ml/model_presets.py`. Do NOT change model IDs or presets unless explicitly asked.

- Embeddings and zero‑shot classification (SigLIP2):
  - Default (M1): `google/siglip2-base-patch16-224` (preset `"m1_default"`).
  - High‑quality (later): `google/siglip2-large-patch16-384` (preset `"hq_384"`).
- Captioning (BLIP):
  - Default (M1): `Salesforce/blip-image-captioning-base` (preset `"caption_base"`).
  - Optional high‑quality: `Salesforce/blip-image-captioning-large` (preset `"caption_large"`).
- Detection (optional in M1):
  - Backend: `owlvit`.
  - Model: `google/owlvit-base-patch32`.
  - M1 can ship with detection disabled (`models.detection.enabled=false`, `pipeline.run_detection=false`).

OCR and few‑shot learning are out of scope for M1 (interfaces only, no concrete OCR implementation).

## Critical Coding Constraints

You MUST respect these constraints (see `AI_CODING_STANDARDS.md` and `docs/AI_CODING_NOTES.md`):

1. Transformers API only
   - Use `AutoProcessor`, `AutoModel`, `AutoModelForSeq2SeqLM`.
   - Do NOT use legacy pipelines such as `pipeline("image-classification", model=...)` or old CLIP APIs for SigLIP.
2. Single model instance per process
   - Use the singleton loaders in `src/vibe_photos/ml/models.py`.
   - Never load models inside tight loops or per image.
3. CPU‑first, GPU‑enhanced
   - Code MUST run correctly on CPU‑only machines.
   - If CUDA/MPS is available, use it via `_select_device("auto")`, but never require it.
4. Config‑driven
   - All model names, presets, batch sizes, and feature flags come from `config/settings.yaml`.
   - Use `Settings` from `src/vibe_photos/config.py` instead of ad‑hoc config parsing.
5. Batching
   - Implement batching for SigLIP and BLIP using `models.embedding.batch_size` and `models.caption.batch_size`.
   - Detection batching is optional in M1.
6. Caching and DB separation
   - Treat caches under `cache/` (JSON/NPY, with model and pipeline version info) as the durable source of truth.
   - SQLite (under `data/`) is a projection over caches and may be rebuilt.
7. Modern Python style
   - Python 3.12, type hints, `dataclasses`, `pathlib.Path`.
   - Follow import order and logging rules from `AI_CODING_STANDARDS.md`.

## High‑Level Tasks for M1

Work in small, testable steps. For each step:

1. Explain which documents/sections you are following.
2. Note which existing modules you are extending.
3. Propose module/function names.
4. Write the code.
5. Add minimal tests or usage examples where feasible.

Priority steps:

1. Config and paths
   - Ensure `Settings` covers input roots, cache directory, DB path, model settings, and pipeline flags.
   - Add a CLI or script to print the effective config.
2. Model loading
   - Reuse `get_siglip_embedding_model()` and `get_blip_caption_model()`.
   - Optionally add `get_detector()` based on `models.detection` config.
3. Preprocessing and hashing
   - Implement a preprocessing module that:
     - Walks configured photo roots.
     - Normalizes images and writes thumbnails under `cache/`.
     - Computes content hashes and perceptual hashes using `xxhash64-v1` and `phash64-v2` as specified in `blueprints/m1/m1_development_plan.md`, and writes them to caches and the appropriate SQLite databases.
4. Embeddings and coarse categories
   - Use SigLIP to compute embeddings in batches and classify coarse categories via `build_siglip_coarse_classifier()`.
5. Captions
   - Use BLIP to generate one short caption per image and store it in caches and SQLite.
6. (Optional) Detection
   - Add a pluggable OWL‑ViT backend controlled by config flags.
7. CLI and status
   - Provide commands to run preprocessing and inspect status for M1.

## Output style

- Prefer clear, well‑structured modules over huge scripts.
- Use docstrings and type hints.
- When adding a new file, always show its relative path.
- When unsure about details not covered by docs, make a reasonable assumption and document it.
```

### 3.2 M2 — Perception Quality & Labeling

Goal: Improve recognition quality and label hygiene on top of the existing M1 pipeline, before investing in full search tooling.

- Label dictionaries and grouping:
  - Iterate on `settings.models.siglip_labels` to:
    - Reduce manual maintenance cost for common creator scenarios (electronics, food, documents, screenshots, products).
    - Improve coverage and grouping for both coarse categories and product‑level hints.
  - Introduce a label blacklist/remapping layer (building on the M1 “Future improvements” section) to suppress low‑information labels and map noisy outputs to more useful categories.
- Detection + SigLIP/BLIP tuning:
  - Refine OWL‑ViT + SigLIP integration:
    - Thresholds for per‑region labels (`siglip_min_prob`, `siglip_min_margin`).
    - Priority heuristics for primary regions and secondary region pruning.
  - Evaluate how well detection + BLIP captions jointly capture “things” users care about (devices, food, documents, screenshots) and adjust prompts/configuration accordingly.
- Evaluation and tooling:
  - Define a small but representative evaluation set (≈1k photos) with:
    - Coarse category labels.
    - Key object/product annotations for a handful of target classes.
  - Add lightweight tools (CLI or notebooks) that:
    - Inspect per‑label distributions and confusion cases.
    - Dump per‑image label/caption summaries for manual review.
  - Capture findings and configuration changes in `blueprints/phase_final/knowledge/lessons_learned.md` and `docs/MODELS.md`.

### 3.3 M3 — Search & Tools (PostgreSQL + pgvector + docker-compose)

Goal: Move to the final database and deployment architecture and expose production search/tools on top of Postgres + pgvector.

- Database:
  - Define PostgreSQL + pgvector schema based on `../specs/database_schema.sql` (adapted to PostgreSQL syntax).
  - Implement migrations via Alembic or a similar tool.
  - Implement a data migration path from SQLite (where feasible).
- Vector search:
  - Create pgvector columns for embeddings and region embeddings.
  - Build indexes and tune parameters for 30k–100k photos.
- `docker-compose` stack:
  - Define services for:
    - PostgreSQL + pgvector.
    - Redis.
    - API.
    - Workers.
    - UI.
  - Provide environment templates (`.env.example`) and simple setup scripts.
- Background processing:
  - Promote ingestion tasks into a proper queue (Celery + Redis).
  - Add retry logic, simple metrics, and error reporting.

### 3.4 M4 — Learning & Personalization

Goal: Turn corrections and examples into a self‑improving system.

- Few‑shot learning:
  - Implement a prototype manager (similar to `few_shot_learner.py` PoC) using real embeddings.
  - UI flow:
    - “Teach a new product” wizard.
    - Show estimated accuracy and thresholds.
- Corrections loop:
  - Log all human corrections and accepted suggestions.
  - Surface frequently overridden labels and patterns.
  - Feed these signals into ranking and few‑shot prototypes.
- Batch operations:
  - Enable “apply label to similar photos” operations.
  - Provide safety checks and undo options where possible.
- Monitoring & Maintenance:
  - Expose basic Prometheus metrics for ingestion throughput and failures.
  - Add backup/restore scripts for PostgreSQL.

## 4. Deployment Notes

- For development:
  - Use `uv` to manage the Python environment.
  - Run preprocessing, search, and evaluation scripts via `uv run` outside Docker in M1–M2.
- For user deployment:
  - Prefer `docker-compose` as the single entry point (M3+).
  - Ensure clear documentation:
    - How to mount photo directories.
    - How to mount `models/`, `cache/`, and `data/`.
    - How to update the stack safely.

Track progress in `AI_TASK_TRACKER.md` and record design decisions and deviations in `decisions/AI_DECISION_RECORD.md`.
