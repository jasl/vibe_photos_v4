# Example configuration for model choices and preprocessing pipeline.
# Copy this file to `config/settings.yaml` and adjust paths and values
# for your local environment. The `./init_project.sh` script will also
# create a baseline `settings.yaml` if it does not exist.

models:
  embedding:
    backend: "siglip"
    model_name: "google/siglip2-base-patch16-224"
    device: "auto"           # "cuda", "mps", or "cpu"
    batch_size: 16

  caption:
    backend: "blip"
    model_name: "Salesforce/blip-image-captioning-base"
    device: "auto"
    batch_size: 4

  detection:
    enabled: true            # M1: detection optional
    backend: "owlvit"        # or "grounding-dino" in future milestones
    model_name: "google/owlvit-base-patch32"
    device: "auto"
    max_regions_per_image: 10
    score_threshold: 0.25
    # SigLIP-based region re-ranking thresholds for per-region labels.
    # ``siglip_min_prob`` is the minimum probability required for the
    # top-1 label within its semantic group. ``siglip_min_margin`` is
    # the required gap between the best and second-best label within
    # that group. Regions that do not meet both thresholds keep only
    # the original detector label (no refined label is written).
    siglip_min_prob: 0.15
    siglip_min_margin: 0.05

  ocr:
    enabled: false           # M1 MUST work with OCR disabled

  # Shared SigLIP label dictionaries for region re-ranking, diagnostics,
  # and prototype detectors. Keeping these in configuration avoids
  # duplicating label lists across modules.
  siglip_labels:
    # Coarse semantic groups for SigLIP labels. Used as the single
    # source of truth for:
    #   - region re-ranking candidate labels,
    #   - mapping detector labels to semantic groups,
    #   - and SimpleDetector label prompts.
    # Downstream code flattens this mapping to build candidate label
    # lists and human-friendly category prompts.
    label_groups:
      food:
        - "food"
        - "tapas"
        - "pizza"
        - "burger"
        - "sushi"
        - "noodles"
        - "dessert"
        - "cake"
      electronics:
        - "phone"
        - "smartphone"
        - "iPhone"
        - "Android phone"
        - "computer"
        - "laptop"
        - "MacBook"
        - "tablet"
        - "iPad"
        - "headphones"
        - "AirPods"
        - "camera"
      document:
        - "document"
        - "book"
        - "notes"
      person:
        - "person"
        - "people"
      scene:
        - "landscape"
        - "architecture"
        - "building"
      animal:
        - "animal"
        - "pet"
      specific_products:
        - "iPhone"
        - "MacBook"
        - "iPad"
        - "AirPods"
        - "Samsung Galaxy"
        - "ThinkPad"
        - "Surface"

pipeline:
  # Whether to run detection step in M1.
  run_detection: true
  # Whether to skip heavy models for near-duplicates.
  skip_duplicates_for_heavy_models: true
  # Hamming distance threshold for near-duplicate grouping (default 12).
  phash_hamming_threshold: 16
  # Target thumbnail size (max width/height in pixels) for web previews.
  thumbnail_size: 512
  # JPEG quality (1â€“100) for thumbnails.
  thumbnail_quality: 85
  # How to store EXIF datetime in the database: "raw" uses the original camera string;
  # "iso" normalizes to an ISO-8601-like "YYYY-MM-DDTHH:MM:SS" format when parsing succeeds.
  exif_datetime_format: "iso"
