# Example configuration for model choices and preprocessing pipeline.
# Copy this file to `config/settings.yaml` and adjust paths and values
# for your local environment. The `./init_project.sh` script will also
# create a baseline `settings.yaml` if it does not exist.

models:
  embedding:
    backend: "siglip"
    model_name: "google/siglip2-base-patch16-224"
    device: "auto"           # "cuda", "mps", or "cpu"
    batch_size: 16

  caption:
    backend: "blip"
    model_name: "Salesforce/blip-image-captioning-base"
    device: "auto"
    batch_size: 4

  detection:
    enabled: true            # M1: detection optional
    backend: "owlvit"        # or "grounding-dino" in future milestones
    model_name: "google/owlvit-base-patch32"
    device: "auto"
    max_regions_per_image: 10
    score_threshold: 0.25

  ocr:
    enabled: false           # M1 MUST work with OCR disabled

pipeline:
  # Whether to run detection step in M1.
  run_detection: true
  # Whether to skip heavy models for near-duplicates.
  skip_duplicates_for_heavy_models: true
  # Hamming distance threshold for near-duplicate grouping (default 12).
  phash_hamming_threshold: 16
  # Target thumbnail size (max width/height in pixels) for web previews.
  thumbnail_size: 512
  # JPEG quality (1â€“100) for thumbnails.
  thumbnail_quality: 85
  # How to store EXIF datetime in the database: "raw" uses the original camera string;
  # "iso" normalizes to an ISO-8601-like "YYYY-MM-DDTHH:MM:SS" format when parsing succeeds.
  exif_datetime_format: "iso"
